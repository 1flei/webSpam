from HTMLParser import HTMLParserimport sys, re, timeimport urllib, zlibfrom nltk.corpus import stopwordsimport numpy as nptagstack = []attrstack = []Dtag = ['script','style']wordlist = []worddict = {}counts = []tops = []#FREQUENTWORD = 5000class MyHTMLParser(HTMLParser):       def __init__(self,count):        self.count = count        HTMLParser.__init__(self)    def handle_starttag(self,tag,attrs):        tagstack.append((tag,len(attrs)))        if len(attrs):            attrstack.extend(attrs)        #print tagstack        #print attrstack        #print '----------------------------------------'#        print "tag:",tag#        sys.stdout.write('<'+tag+' '+str(attrs)+'>')    def handle_endtag(self,tag):        attrsnum = tagstack.pop()[1]        for i in range(attrsnum):            attrstack.pop()    def handle_data(self,data):##        with open('../data/file/'+str(self.count)+".txt", 'a+') as f:##            f.write(data)        if data.strip() != '':                        s = tagstack[-1]            flag = 1 if s[0] in Dtag else 0            data = data.strip()            if flag == 0:                #print data                tmp = re.split('[!"#&\'()*+,-./:;<=>?@[\\]^_`{|}~\s]+',data)                tmpcount = 0                for i in range(len(tmp)):                    if tmp[i] != '':                        tmpcount += 1                        buildCorpus(tmp[i])                    #print str(self.count)+' '+str(data)##                print self.count,##                print tagstack[-1],##                print datavalData = "../data/webspam_val.dat/webspam_val.dat"#trainData = "../data/test.txt"trainData = "webspam_train.dat" #"../data/webspam_val.dat/webspam_val.dat"outpFilePath = "../data/webspam_splite.dat"def buildCorpus(string):    if string not in stopwords.words('english'):        if string not in wordlist:            #wordlist.update({string:1})            #counts.append(1)            #wordlist.append(string)            worddict[string] = len(wordlist)            wordlist.append(string)            counts.append(1)        else:            counts[worddict[string]] += 1            #wordlist[string] += 1            #counts[wordlist.index(string)] += 1def computeTops():    order = (-np.array(counts)).argsort()    tops = [(wordlist[order[i]],counts[order[i]]) for i in range(len(order))]    return topsdef writeOutFile(filePath):    outFile = open(filePath,"w")    for i in range(len(tops)):        outFile.write(tops[i][0]+' '+str(tops[i][1])+'\n')    outFile.close()def readInpFile(filePath):    count=0    inpFile = open(filePath,"rb")    html = ''    start = time.time()    while True:        parser = MyHTMLParser(count)        s = inpFile.read(1)        if s == '\0':##            with open('../data/file/'+str(count)+".txt", 'a+') as f:##                f.write(html)            #print html            print count            parser.feed(html)            html = ''            #print parser.charcount            # compute feature vector            count += 1            if count % 100 == 0:                print time.time()-start        else:            html += s        if s == '':            break    tops.extend(computeTops())    print tops[0]    returnif __name__ == "__main__":    readInpFile(trainData)    writeOutFile('freq_word.txt')    #inpFile = open(inpFilePath,"r")    #outpFile = open(outpFilePath,"w")       #s=inpFile.read(300000)    #outpFile.write(s)